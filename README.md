# Sign Language Translation with Speech Synthesis

## Overview
This project aims to translate sign language gestures into spoken words using a combination of computer vision and machine learning techniques. It utilizes OpenCV and MediaPipe for collecting action (sign) keypoints from the user, which are then passed to an LSTM (Long Short-Term Memory) model for predicting the word associated with the action. The predicted words are then synthesized into speech.

![Example](example.jpeg)
